{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.request import urlopen\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "import time\n",
    "# nltk.download() # Might need this if tokenize doens't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading book: http://www.gutenberg.org/files/28054/28054-0.txt\n",
      "Cleaning sentences...\n",
      "Finished Cleaning\n"
     ]
    }
   ],
   "source": [
    "# Creating word and sentence tokens\n",
    "bookURLs = [\"http://www.gutenberg.org/files/28054/28054-0.txt\"] # takes about 1 hour per book on my cpu to encode 1 hots\n",
    "\n",
    "#             \"http://www.gutenberg.org/files/2554/2554-0.txt\", \n",
    "#             \"http://www.gutenberg.org/files/2600/2600-0.txt\",\n",
    "#             \"http://www.gutenberg.org/files/1399/1399-0.txt\",\n",
    "#             \"http://www.gutenberg.org/files/98/98-0.txt\",\n",
    "#             \"http://www.gutenberg.org/files/1400/1400-0.txt\",\n",
    "#             \"http://www.gutenberg.org/cache/epub/730/pg730.txt\",\n",
    "#             \"http://www.gutenberg.org/cache/epub/84/pg84.txt\",\n",
    "#             \"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\",\n",
    "#             \"http://www.gutenberg.org/cache/epub/7849/pg7849.txt\"\n",
    "\n",
    "wordTokens = []\n",
    "sentTokens = []\n",
    "for book in bookURLs:\n",
    "    print(\"Reading book: \" + book)\n",
    "    response = urlopen(book)\n",
    "    rawbook = response.read().decode('utf8')\n",
    "    wordTokens += word_tokenize(rawbook)\n",
    "    sentTokens += sent_tokenize(rawbook)\n",
    "    \n",
    "# Cleaning sentences\n",
    "print(\"Cleaning sentences...\")\n",
    "for i in range(len(sentTokens)):\n",
    "    sentTokens[i] = re.sub(r'\\r*\\n', \" \", sentTokens[i])\n",
    "    sentTokens[i] = re.sub(r' +', \" \", sentTokens[i])\n",
    "print(\"Finished Cleaning\")\n",
    "\n",
    "# Creating one-hot words\n",
    "wordlb = preprocessing.LabelBinarizer()\n",
    "wordEncoding = wordlb.fit_transform(wordTokens)\n",
    "print(\"One-hot word dimensions:\", wordEncoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentences completed of: 19843 in: 0.41724586486816406 seconds\n",
      "ETA: 8279.40969657898\n",
      "50 sentences completed of: 19843 in: 5.066877126693726 seconds\n",
      "ETA: 1966.4450778166452\n",
      "100 sentences completed of: 19843 in: 8.56989574432373 seconds\n",
      "ETA: 1675.202491883004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-3cee4df799e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msentOfOneHotWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'START'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordsInSent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moneHotWord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0msentOfOneHotWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneHotWord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentOfOneHotWords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxSentenceLength\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pasha/anaconda3/lib/python3.4/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    333\u001b[0m                               \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                               \u001b[0mneg_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                               sparse_output=self.sparse_output)\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pasha/anaconda3/lib/python3.4/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mlabel_binarize\u001b[0;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0msorted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         raise ValueError(\"classes {0} missmatch with the labels {1}\"\n",
      "\u001b[0;32m/Users/Pasha/anaconda3/lib/python3.4/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creates an array called allSentences: each element is an array whose elements are one-hot words\n",
    "# First element is \"START\", Last element is \"END\", padded with \"PAD\" until sentence is maxSentenceLength\n",
    "# Otherwise it is truncated at maxSentenceLength and has no padding\n",
    "# Dimensions of allSentences: numSentences * maxSentenceLength * numWords <-- number of unique words\n",
    "\n",
    "allSentences = []\n",
    "i = 0\n",
    "maxSentenceLength = 25\n",
    "numSentences = len(sentTokens)\n",
    "averageTime = []\n",
    "for sent in sentTokens:\n",
    "    startTime = time.time()\n",
    "    wordsInSent = word_tokenize(sent)\n",
    "    sentOfOneHotWords = ['START']\n",
    "    for word in wordsInSent:\n",
    "        oneHotWord = wordlb.transform([word])[0]\n",
    "        sentOfOneHotWords.append(oneHotWord)\n",
    "    while len(sentOfOneHotWords) < (maxSentenceLength + 1):\n",
    "        sentOfOneHotWords.append([\"PAD\"])\n",
    "    sentOfOneHotWords = sentOfOneHotWords[:(maxSentenceLength + 1)] # truncating (+1 to account for \"START\" Token)\n",
    "    sentOfOneHotWords.append(['END'])\n",
    "    allSentences.append(sentOfOneHotWords)\n",
    "    averageTime.append(time.time() - startTime)\n",
    "    if i % 50 == 0:\n",
    "        print(i, \"sentences completed of:\", numSentences, \"in:\", sum(averageTime), \"seconds\")\n",
    "        print(\"ETA:\", (sum(averageTime)/len(averageTime))*(numSentences-sentTokens.index(sent)), \"seconds\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns sentence at specified index with one-hot encoded words\n",
    "def getSentence(sentenceIndex):\n",
    "    if sentenceIndex > len(allSentences):\n",
    "        raise ValueError(\"Sentence index is greater number of sentences in corpus\")\n",
    "    return(allSentences[sentenceIndex])\n",
    "\n",
    "# Returns one-hot encoded word at specified sentence and word indices\n",
    "def getWord(sentenceIndex, wordIndex):\n",
    "    if wordIndex > maxSentenceLength+1:\n",
    "        raise ValueError(\"Word index is greater than max sentence length\")\n",
    "    if allSentences[sentenceIndex][wordIndex][0] == \"START\":\n",
    "        return(\"START\")\n",
    "    elif allSentences[sentenceIndex][wordIndex][0] == \"END\":\n",
    "        return(\"END\")\n",
    "    elif allSentences[sentenceIndex][wordIndex][0] == \"PAD\":\n",
    "        return(\"PAD\")\n",
    "    numWords = wordEncoding.shape[1]\n",
    "    return(np.array(getSentence(sentenceIndex)[wordIndex]).reshape(1, numWords))\n",
    "\n",
    "# Decodes word at specified sentence and word indicies back into English\n",
    "def decode(sentenceIndex, wordIndex):\n",
    "    word = getWord(sentenceIndex, wordIndex)\n",
    "    if type(word) == str: # 'START', 'END', 'PAD'\n",
    "        return word\n",
    "    return(wordlb.inverse_transform(word)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
