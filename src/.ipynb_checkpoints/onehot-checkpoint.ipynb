{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.request import urlopen\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import unicodedata\n",
    "\n",
    "# nltk.download() # Might need this if tokenize doens't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class data: # initialize with data([\"bookURL1\", \"bookURL2\"...])\n",
    "    \n",
    "    def cleanData(self, bookURLs):\n",
    "        wordTokens = ['ppaadd']\n",
    "        sentTokens = []\n",
    "        for book in bookURLs:\n",
    "            print(\"Reading book: \" + book)\n",
    "            response = urlopen(book)\n",
    "            rawbook = response.read().decode('utf8')\n",
    "            rawbook =  re.sub(r'\\r*\\n', \" \", rawbook)\n",
    "            rawbook = re.sub(r' +', \" \", rawbook)\n",
    "            rawbook = rawbook.replace('”','\"').replace('“', '\"')\n",
    "            rawbook = rawbook.replace('\" \"', ' Quote. Quote ')\n",
    "            rawbook = rawbook.replace('\"\\n\"', ' Quote. Quote ')\n",
    "            rawbook = re.sub(r'\"', ' Quote ', rawbook)\n",
    "            rawbook = rawbook.lower()\n",
    "            wordTokens += word_tokenize(rawbook)\n",
    "            sentTokens += sent_tokenize(rawbook)\n",
    "        print(\"Cleaning sentences...\")\n",
    "        for i in range(len(sentTokens)):\n",
    "            sentTokens[i] = word_tokenize(sentTokens[i])\n",
    "        print(\"Finished Cleaning\")\n",
    "\n",
    "        # Creating one-hot words\n",
    "        wordTokens = [x.lower() for x in wordTokens] # makes all words lowercase\n",
    "        wordTokens = list(set(wordTokens))\n",
    "        wordTokens = sorted(wordTokens)\n",
    "        wordlb = preprocessing.LabelBinarizer()\n",
    "        wordEncoding = wordlb.fit_transform(wordTokens)\n",
    "        print(\"Number of unique words:\", wordEncoding.shape[0])\n",
    "        print('Cleaning Data Complete')\n",
    "        return wordlb, wordEncoding, wordTokens, sentTokens #### wordlb, encoding, tokens, ppaadd\n",
    "    \n",
    "    def createDicts(self):\n",
    "        print('Creating Dictionaries...')\n",
    "        encodeDict = {} #{'this' : 5} if the one hot is 00001000...\n",
    "        decodeDict = {}\n",
    "        print('Creating dicts')\n",
    "        for i in range(len(self.wordTokens)):\n",
    "            word = self.wordTokens[i]\n",
    "            index_of_1 = i\n",
    "            encodeDict[word] = index_of_1\n",
    "            decodeDict[index_of_1] = word\n",
    "        print('Created dictionaries')\n",
    "        return encodeDict, decodeDict\n",
    "       \n",
    "        \n",
    "    def getAllSentences(self):\n",
    "        print('Creating all sentences...')\n",
    "        allSentences = []\n",
    "        for sent in self.sentTokens:\n",
    "            sentOfOneHotWords = []\n",
    "            for word in sent:\n",
    "                index = self.word_to_index(word)\n",
    "                sentOfOneHotWords.append(index)\n",
    "            while len(sentOfOneHotWords) < (self.maxSentenceLength + 1):\n",
    "                sentOfOneHotWords.append(self.word_to_index('ppaadd'))\n",
    "            sentOfOneHotWords = sentOfOneHotWords[:(self.maxSentenceLength)]\n",
    "            allSentences.append(sentOfOneHotWords)\n",
    "        print('Done creating sentences')\n",
    "        return allSentences\n",
    "              \n",
    "    def word_to_index(self, word):\n",
    "        return self.encodeDict[word]\n",
    "\n",
    "    # maps index of the 1 to actual onehot encoding\n",
    "    def index_to_onehot(self, index):\n",
    "        onehot = np.append(np.append(np.zeros(index), 1), np.zeros(self.num_unique_words - (index+1)))\n",
    "        onehot = onehot.reshape(1, len(onehot))\n",
    "        return onehot \n",
    "    \n",
    "    def getSentence(self, sentenceIndex):\n",
    "        if sentenceIndex > len(self.allSentences):\n",
    "            raise ValueError(\"Sentence index is greater number of sentences in corpus\")\n",
    "        return(self.allSentences[sentenceIndex])\n",
    "\n",
    "    # Returns sentence with all words in onehot\n",
    "    def getOneHotSentence(self, sentenceIndex):\n",
    "        sentence = self.getSentence(sentenceIndex)\n",
    "        onehotSentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            onehotSentence.append(self.getWord(sentenceIndex, i))\n",
    "        return(onehotSentence)\n",
    "\n",
    "    # Returns [[00...000]], a one-hot encoded word at specified sentence and word index in a nested array (for decoding)\n",
    "    def getWord(self, sentenceIndex, wordIndex):\n",
    "        if wordIndex > self.maxSentenceLength+1:\n",
    "            raise ValueError(\"Word index is greater than max sentence length\")\n",
    "        word = self.allSentences[sentenceIndex][wordIndex]\n",
    "        if type(word) != int:\n",
    "            return word\n",
    "        else:\n",
    "            return(self.index_to_onehot(self.getSentence(sentenceIndex)[wordIndex])) # returns [[000...000]]\n",
    "\n",
    "    # Decodes word at specified sentence and word indicies back into English\n",
    "    def decode(self, sentenceIndex, wordIndex):\n",
    "        word = self.getWord(sentenceIndex, wordIndex)\n",
    "        if type(word) == str: # 'START', 'END', 'PAD'\n",
    "            return word\n",
    "        return(self.wordlb.inverse_transform(word)[0])\n",
    "    \n",
    "    def one_hot_to_word(self, onehot):\n",
    "        return self.wordlb.inverse_transform(onehot)[0]\n",
    "    \n",
    "    def one_hot_sentence_to_sentence(self, sent):\n",
    "        real = []\n",
    "        for word in sent:\n",
    "            real.append(self.one_hot_to_word(word))\n",
    "        return real\n",
    "\n",
    "    # returns numSentences random sentences with words in onehot\n",
    "    def getBatch(self, numSentences):\n",
    "        batch = []\n",
    "        for i in range(numSentences):\n",
    "            rand = np.random.random_integers(len(self.allSentences))\n",
    "            batch.append(self.getOneHotSentence(rand))\n",
    "        return batch\n",
    "    \n",
    "    def __init__(self, bookURLs, encodeDict=None, decodeDict=None):\n",
    "        self.maxSentenceLength = 100\n",
    "        self.bookURLs = bookURLs\n",
    "        self.wordlb, self.wordEncoding, self.wordTokens, self.sentTokens = self.cleanData(self.bookURLs)\n",
    "        self.encodeDict = {} #{'this' : 5} if the one hot is 00001000...\n",
    "        self.decodeDict = {} # {5: 'this}\n",
    "        if ((encodeDict != None) and (decodeDict != None)):\n",
    "            self.encodeDict, self.decodeDict = encodeDict, decodeDict\n",
    "        else:\n",
    "            self.encodeDict, self.decodeDict = self.createDicts()\n",
    "        self.num_unique_words = len(self.decodeDict)\n",
    "        self.allSentences = self.getAllSentences()\n",
    "        print('Data initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading book: http://www.gutenberg.org/files/28054/28054-0.txt\n",
      "Finished Cleaning\n",
      "Number of unique words: 14809\n",
      "Cleaning Data Complete\n",
      "Creating Dictionaries...\n",
      "Creating dicts\n",
      "Created dictionaries\n",
      "Creating all sentences...\n",
      "Done creating sentences\n",
      "Data initialized\n"
     ]
    }
   ],
   "source": [
    "test = data([\"http://www.gutenberg.org/files/28054/28054-0.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'easier'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Should print the same thing twice\n",
    "num = 4000\n",
    "print(test.wordTokens[num])\n",
    "test.wordlb.inverse_transform(np.append(np.append(np.zeros(num), 1), np.zeros(16359- (num+1))).reshape(1, 16359))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quote', 'i', 'had', 'no', 'thoughts', 'for', 'either', 'of', 'them', 'all', 'this', 'last', 'month', '.', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd', 'ppaadd']\n"
     ]
    }
   ],
   "source": [
    "a=test.one_hot_sentence_to_sentence(test.getBatch(5)[0])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25914"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.sentTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
